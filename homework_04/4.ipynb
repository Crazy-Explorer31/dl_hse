{"cells":[{"cell_type":"markdown","id":"3817cdf3-6f92-4d56-807d-d67a187ad617","metadata":{"id":"3817cdf3-6f92-4d56-807d-d67a187ad617"},"source":["<center><h1>Основы глубокого обучение</h1></center>"]},{"cell_type":"code","execution_count":1,"id":"c536aa43-b3b7-4421-b12c-cad3d20a46ef","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c536aa43-b3b7-4421-b12c-cad3d20a46ef","executionInfo":{"status":"ok","timestamp":1762508753254,"user_tz":-180,"elapsed":153,"user":{"displayName":"Степан Соколовский","userId":"12665475918766064375"}},"outputId":"1430e490-17e7-46bb-d49f-4fc4829c2b48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.12.12\n"]}],"source":["!python -V # Версия Python"]},{"cell_type":"code","execution_count":null,"id":"794dc847-eb8f-4f73-a769-5c42c406a6e1","metadata":{"id":"794dc847-eb8f-4f73-a769-5c42c406a6e1"},"outputs":[],"source":["# Подавление предупреждений\n","import warnings\n","for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n","\n","# Импорт необходимых библиотек\n","import os\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import polars as pl\n","import pandas as pd\n","import sklearn\n","import networkx as nx\n","import ipywidgets\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.dates as mdates\n","\n","from torch import Tensor\n","from einops import rearrange\n","from typing import Tuple, Callable\n","from torch.autograd import Function\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import load_iris\n","from sklearn.preprocessing import MinMaxScaler\n","from mpl_toolkits.mplot3d import Axes3D"]},{"cell_type":"code","execution_count":null,"id":"5f74cb53-2804-49f6-a502-e92a6399c25c","metadata":{"id":"5f74cb53-2804-49f6-a502-e92a6399c25c"},"outputs":[],"source":["# Версии необходимых библиотек\n","packages = [\n","    \"Torch\", \"NumPy\", \"Polars\", \"Pandas\", \"Matplotlib\", \"Scikit-learn\", \"Ipywidgets\", \"JupyterLab\"\n","]\n","\n","package_objects = [\n","    torch, np, pl, pd, mpl, sklearn, ipywidgets\n","]\n","\n","versions = list(map(lambda obj: obj.__version__, package_objects))\n","\n","columns_order = [\"№\", \"Библиотека\", \"Версия\"]\n","df_pkgs = (\n","    pl.DataFrame({\n","        columns_order[1]: packages,\n","        columns_order[2]: versions\n","    })\n","    .with_columns(pl.arange(1, pl.lit(len(packages)) + 1).alias(columns_order[0]))\n","    .select(columns_order)\n",")\n","\n","display(df_pkgs)\n","\n","path2reqs = \".\"\n","reqs_name = \"requirements.txt\"\n","\n","def get_packages_and_versions():\n","    \"\"\"Генерация строк с библиотеками и их версиями в формате: библиотека==версия\"\"\"\n","\n","    for package, version in zip(packages, versions):\n","        yield f\"{package.lower()}=={version}\\n\"\n","\n","with open(os.path.join(path2reqs, reqs_name), \"w\", encoding = \"utf-8\") as f:\n","    f.writelines(get_packages_and_versions())"]},{"cell_type":"markdown","id":"ec61dc6d-db97-4798-84c9-d4c4844f8275","metadata":{"id":"ec61dc6d-db97-4798-84c9-d4c4844f8275"},"source":["# Лекция 4\n","\n","4. **Современные архитектуры глубокого обучения**\n","    - Обзор семейства xLSTM\n","    - Применение данных архитектур для анализа последовательных данных, изображений и мультимодальной интеграции"]},{"cell_type":"markdown","id":"9e1fb0c1-bc89-4c3a-804c-b40e2e5068eb","metadata":{"id":"9e1fb0c1-bc89-4c3a-804c-b40e2e5068eb"},"source":["## Вспоминаем LSTM\n","\n","<div style=\"text-align: center;\">\n","    <img src=\"imgs/6_1.png\" alt=\"\" width=\"800px\">\n","</div>\n","\n","### Основные моменты\n","\n","<div style=\"text-align: center;\">\n","    <img src=\"imgs/6_22.png\" alt=\"\" width=\"500px\">\n","</div>\n","\n","### Шаг 1\n","\n","<div style=\"text-align: center;\">\n","    <img src=\"imgs/6_3.png\" alt=\"\" width=\"500px\">\n","</div>\n","\n","$$\n","f_t=\\sigma\\left(W_f \\times\\left[h_{t-1}, x_t\\right]+b_f\\right)\n","$$\n","\n","### Шаг 2\n","\n","<div style=\"text-align: center;\">\n","    <img src=\"imgs/6_4.png\" alt=\"\" width=\"500px\">\n","</div>\n","\n","$$\n","\\begin{aligned}\n","i_t & =\\sigma\\left(W_i \\times\\left[h_{t-1}, x_t\\right]+b_i\\right) \\\\\n","\\tilde{C}_t & =\\tanh \\left(W_C \\times\\left[h_{t-1}, x_t\\right]+b_C\\right)\n","\\end{aligned}\n","$$\n","\n","### Шаг 3\n","\n","<div style=\"text-align: center;\">\n","    <img src=\"imgs/6_5.png\" alt=\"\" width=\"500px\">\n","</div>\n","\n","$$\n","C_t=f_t * C_{t-1}+i_t * \\tilde{C}_t\n","$$\n","\n","### Шаг 4\n","\n","<div style=\"text-align: center;\">\n","    <img src=\"imgs/6_6.png\" alt=\"\" width=\"500px\">\n","</div>\n","\n","$$\n","\\begin{aligned}\n","& o_t=\\sigma\\left(W_o\\left[h_{t-1}, x_t\\right]+b_o\\right) \\\\\n","& h_t=o_t * \\tanh \\left(C_t\\right)\n","\\end{aligned}\n","$$\n","\n","### Проблемы LSTM\n","\n","1. Ограниченная способность пересматривать решения о хранении информации\n","2. Ограниченная способность хранить информацию\n","3. Невозможно распараллеливать вычисления"]},{"cell_type":"markdown","id":"3581e287-7663-4406-afb2-2756d9a60423","metadata":{"id":"3581e287-7663-4406-afb2-2756d9a60423"},"source":["## sLSTM\n","\n","<div style=\"text-align: center;\">\n","    <img src=\"imgs/6_7.png\" alt=\"\" width=\"900px\">\n","</div>\n","\n","$$\n","n_t=\\mathrm{f}_t n_{t-1}+\\mathrm{i}_t\n","$$\n","\n","### Способ вычисления выходов гейтов забывания и входного состояния\n","\n","#### LSTM\n","\n","$$\n","\\begin{array}{llr}\n","\\mathrm{i}_t=\\sigma\\left(\\tilde{\\mathrm{i}}_t\\right), & \\tilde{\\mathrm{i}}_t=\\boldsymbol{w}_{\\mathrm{i}}^{\\top} \\boldsymbol{x}_t+r_{\\mathrm{i}} h_{t-1}+b_{\\mathrm{i}} & \\text { input gate } \\\\\n","\\mathrm{f}_t=\\sigma\\left(\\tilde{\\mathrm{f}}_t\\right), & \\tilde{\\mathrm{f}}_t=\\boldsymbol{w}_{\\mathrm{f}}^{\\top} \\boldsymbol{x}_t+r_{\\mathrm{f}} h_{t-1}+b_{\\mathrm{f}} & \\text { forget gate }\n","\\end{array}\n","$$\n","\n","#### sLSTM\n","\n","$$\n","\\begin{array}{rlr}\n","m_t & =\\max \\left(\\log \\left(\\mathrm{f}_t\\right)+m_{t-1}, \\log \\left(\\mathrm{i}_t\\right)\\right) & \\text { stabilizer state } \\\\\n","\\mathrm{i}_t^{\\prime} & =\\exp \\left(\\log \\left(\\mathrm{i}_t\\right)-m_t\\right)=\\exp \\left(\\tilde{\\mathrm{i}}_t-m_t\\right) & \\text { stabil. input gate } \\\\\n","\\mathrm{f}_t^{\\prime} & =\\exp \\left(\\log \\left(\\mathrm{f}_t\\right)+m_{t-1}-m_t\\right) & \\text { stabil. forget gate }\n","\\end{array}\n","$$\n","\n","#### mLSTM\n","\n","##### Правило обновления ковариаций\n","\n","$$\n","\\boldsymbol{C}_t=\\boldsymbol{C}_{t-1}+\\boldsymbol{v}_t \\boldsymbol{k}_t^{\\top}\n","$$\n","\n","##### Гейты вычисляются также с помощью экспоненты, но без использования предыдущих скрытых состояний\n","\n","###### LSTM\n","\n","$$\n","\\begin{array}{llr}\n","\\mathrm{i}_t=\\sigma\\left(\\tilde{\\mathrm{i}}_t\\right), & \\tilde{\\mathrm{i}}_t=\\boldsymbol{w}_{\\mathrm{i}}^{\\top} \\boldsymbol{x}_t+\\underline{r_{\\mathrm{i}} h_{t-1}}+b_{\\mathrm{i}} & \\text { input gate } \\\\\n","\\mathrm{f}_t=\\sigma\\left(\\tilde{\\mathrm{f}}_t\\right), & \\tilde{\\mathrm{f}}_t=\\boldsymbol{w}_{\\mathrm{f}}^{\\top} \\boldsymbol{x}_t+\\underline{r_{\\mathrm{f}} h_{t-1}}+b_{\\mathrm{f}} & \\text { forget gate } \\\\\n","\\mathrm{o}_t=\\sigma\\left(\\tilde{\\mathrm{o}}_t\\right), & \\tilde{\\mathrm{o}}_t=\\boldsymbol{w}_{\\mathrm{o}}^{\\top} \\boldsymbol{x}_t+\\underline{r_{\\mathrm{o}} h_{t-1}}+b_{\\mathrm{o}} & \\text { output gate }\n","\\end{array}\n","$$\n","\n","###### mLSTM\n","\n","$$\n","\\begin{array}{lrl}\n","\\mathrm{i}_t=\\exp \\left(\\tilde{\\mathrm{i}}_t\\right), & \\tilde{\\mathrm{i}}_t=\\boldsymbol{w}_{\\mathrm{i}}^{\\top} \\boldsymbol{x}_t+b_{\\mathrm{i}} & \\text { input gate } \\\\\n","\\mathrm{f}_t=\\sigma\\left(\\tilde{\\mathrm{f}}_t\\right) \\text { OR } \\exp \\left(\\tilde{\\mathrm{f}}_t\\right), & \\tilde{\\mathrm{f}}_t=\\boldsymbol{w}_{\\mathrm{f}}^{\\top} \\boldsymbol{x}_t+b_{\\mathrm{f}} & \\text { forget gate } \\\\\n","\\mathbf{o}_t=\\sigma\\left(\\tilde{\\mathbf{o}}_t\\right), & \\tilde{\\mathbf{o}}_t=\\boldsymbol{W}_{\\mathbf{o}} \\boldsymbol{x}_t+\\boldsymbol{b}_{\\mathbf{o}} & \\text { output gate }\n","\\end{array}\n","$$\n","\n","##### Нормализация скрытого состояния\n","\n","$$\n","\\boldsymbol{h}_t=\\mathbf{o}_t \\odot \\tilde{\\boldsymbol{h}}_t, \\quad \\quad \\tilde{\\boldsymbol{h}}_t=\\boldsymbol{C}_t \\boldsymbol{q}_t / \\max \\left\\{\\left|\\boldsymbol{n}_t^{\\top} \\boldsymbol{q}_t\\right|, 1\\right\\} \\quad \\text { hidden state }\n","$$\n","\n","$$\n","\\begin{aligned}\n","\\boldsymbol{q}_t & =\\boldsymbol{W}_q \\boldsymbol{x}_t+\\boldsymbol{b}_q \\\\\n","\\boldsymbol{k}_t & =\\frac{1}{\\sqrt{d}} \\boldsymbol{W}_k \\boldsymbol{x}_t+\\boldsymbol{b}_k \\\\\n","\\boldsymbol{v}_t & =\\boldsymbol{W}_v \\boldsymbol{x}_t+\\boldsymbol{b}_v\n","\\end{aligned}\n","$$"]},{"cell_type":"markdown","id":"30288ba0-f5cd-4dfe-b83e-6f958d8efba2","metadata":{"id":"30288ba0-f5cd-4dfe-b83e-6f958d8efba2"},"source":["## Vision-LSTM\n","\n","<div style=\"text-align: center;\">\n","    <img src=\"imgs/6_8.png\" alt=\"\" width=\"900px\">\n","</div>"]},{"cell_type":"markdown","id":"aa28bf0f-9c78-48bc-b4e1-6db272bed9dc","metadata":{"id":"aa28bf0f-9c78-48bc-b4e1-6db272bed9dc"},"source":["# Семинар 4"]},{"cell_type":"code","execution_count":null,"id":"dd54170a-8ef2-460e-b390-1634341006a2","metadata":{"id":"dd54170a-8ef2-460e-b390-1634341006a2"},"outputs":[],"source":["class CausalConv1D(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, **kwargs):\n","        super(CausalConv1D, self).__init__()\n","        self.padding = (kernel_size - 1) * dilation\n","        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=self.padding, dilation=dilation, **kwargs)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x[:, :, :-self.padding]\n","\n","class BlockDiagonal(nn.Module):\n","    def __init__(self, in_features, out_features, num_blocks):\n","        super(BlockDiagonal, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.num_blocks = num_blocks\n","\n","        assert out_features % num_blocks == 0\n","\n","        block_out_features = out_features // num_blocks\n","\n","        self.blocks = nn.ModuleList([\n","            nn.Linear(in_features, block_out_features)\n","            for _ in range(num_blocks)\n","        ])\n","\n","    def forward(self, x):\n","        x = [block(x) for block in self.blocks]\n","        x = torch.cat(x, dim=-1)\n","        return x\n","\n","class sLSTMBlock(nn.Module):\n","    def __init__(self, input_size, head_size, num_heads, proj_factor=4/3):\n","        super(sLSTMBlock, self).__init__()\n","        self.input_size = input_size\n","        self.head_size = head_size\n","        self.hidden_size = head_size * num_heads\n","        self.num_heads = num_heads\n","        self.proj_factor = proj_factor\n","\n","        assert proj_factor > 0\n","\n","        self.layer_norm = nn.LayerNorm(input_size)\n","        self.causal_conv = CausalConv1D(1, 1, 4)\n","\n","        self.Wz = BlockDiagonal(input_size, self.hidden_size, num_heads)\n","        self.Wi = BlockDiagonal(input_size, self.hidden_size, num_heads)\n","        self.Wf = BlockDiagonal(input_size, self.hidden_size, num_heads)\n","        self.Wo = BlockDiagonal(input_size, self.hidden_size, num_heads)\n","\n","        self.Rz = BlockDiagonal(self.hidden_size, self.hidden_size, num_heads)\n","        self.Ri = BlockDiagonal(self.hidden_size, self.hidden_size, num_heads)\n","        self.Rf = BlockDiagonal(self.hidden_size, self.hidden_size, num_heads)\n","        self.Ro = BlockDiagonal(self.hidden_size, self.hidden_size, num_heads)\n","\n","        self.group_norm = nn.GroupNorm(num_heads, self.hidden_size)\n","\n","        self.up_proj_left = nn.Linear(self.hidden_size, int(self.hidden_size * proj_factor))\n","        self.up_proj_right = nn.Linear(self.hidden_size, int(self.hidden_size * proj_factor))\n","        self.down_proj = nn.Linear(int(self.hidden_size * proj_factor), input_size)\n","\n","    def forward(self, x, prev_state):\n","        assert x.size(-1) == self.input_size\n","        h_prev, c_prev, n_prev, m_prev = prev_state\n","\n","        h_prev = h_prev.to(x.device)\n","        c_prev = c_prev.to(x.device)\n","        n_prev = n_prev.to(x.device)\n","        m_prev = m_prev.to(x.device)\n","\n","        x_norm = self.layer_norm(x)\n","        x_conv = F.silu(self.causal_conv(x_norm.unsqueeze(1)).squeeze(1))\n","\n","        z = torch.tanh(self.Wz(x_norm) + self.Rz(h_prev))\n","        o = torch.sigmoid(self.Wo(x_norm) + self.Ro(h_prev))\n","        i_tilde = self.Wi(x_conv) + self.Ri(h_prev)\n","        f_tilde = self.Wf(x_conv) + self.Rf(h_prev)\n","\n","        # TODO !!!!\n","        m_t = torch.max(f_tilde + m_prev, i_tilde)\n","        i = torch.exp(i_tilde - m_t) # Всегда <= 1.0\n","        f = torch.exp(f_tilde + m_prev - m_t) # Всегда <= 1.0\n","\n","        c_t = f * c_prev + i * z\n","        n_t = f * n_prev + i\n","        h_t = o * c_t / n_t\n","\n","        output = h_t\n","        output_norm = self.group_norm(output)\n","        output_left = self.up_proj_left(output_norm)\n","        output_right = self.up_proj_right(output_norm)\n","        output_gated = F.gelu(output_right)\n","        output = output_left * output_gated\n","        output = self.down_proj(output)\n","        final_output = output + x\n","\n","        return final_output, (h_t, c_t, n_t, m_t)\n","\n","class sLSTM(nn.Module):\n","    # Add bias, dropout, bidirectional\n","    def __init__(self, input_size, head_size, num_heads, num_layers=1, batch_first=False, proj_factor=4/3):\n","        super(sLSTM, self).__init__()\n","        self.input_size = input_size\n","        self.head_size = head_size\n","        self.hidden_size = head_size * num_heads\n","        self.num_heads = num_heads\n","        self.num_layers = num_layers\n","        self.batch_first = batch_first\n","        self.proj_factor_slstm = proj_factor\n","\n","        self.layers = nn.ModuleList([sLSTMBlock(input_size, head_size, num_heads, proj_factor) for _ in range(num_layers)])\n","\n","    def forward(self, x, state=None):\n","        assert x.ndim == 3\n","        if self.batch_first: x = x.transpose(0, 1)\n","        seq_len, batch_size, _ = x.size()\n","\n","        if state is not None:\n","            state = torch.stack(list(state)).to(x.device)\n","            assert state.ndim == 4\n","            num_hidden, state_num_layers, state_batch_size, state_input_size = state.size()\n","            assert num_hidden == 4\n","            assert state_num_layers == self.num_layers\n","            assert state_batch_size == batch_size\n","            assert state_input_size == self.input_size\n","            state = state.transpose(0, 1)\n","        else:\n","            state = torch.zeros(self.num_layers, 4, batch_size, self.hidden_size, device=x.device)\n","\n","        output = []\n","        for t in range(seq_len):\n","            x_t = x[t]\n","            for layer in range(self.num_layers):\n","                x_t, state_tuple = self.layers[layer](x_t, tuple(state[layer].clone()))\n","                state[layer] = torch.stack(list(state_tuple))\n","            output.append(x_t)\n","\n","        output = torch.stack(output)\n","        if self.batch_first:\n","            output = output.transpose(0, 1)\n","        state = tuple(state.transpose(0, 1))\n","        return output, state\n","\n","class mLSTMBlock(nn.Module):\n","    def __init__(self, input_size, head_size, num_heads, proj_factor=2):\n","        super(mLSTMBlock, self).__init__()\n","        self.input_size = input_size\n","        self.head_size = head_size\n","        self.hidden_size = head_size * num_heads\n","        self.num_heads = num_heads\n","        self.proj_factor = proj_factor\n","\n","        assert proj_factor > 0\n","\n","        self.layer_norm = nn.LayerNorm(input_size)\n","        self.up_proj_left = nn.Linear(input_size, int(input_size * proj_factor))\n","        self.up_proj_right = nn.Linear(input_size, self.hidden_size)\n","        self.down_proj = nn.Linear(self.hidden_size, input_size)\n","\n","        self.causal_conv = CausalConv1D(1, 1, 4)\n","        self.skip_connection = nn.Linear(int(input_size * proj_factor), self.hidden_size)\n","\n","        self.Wq = BlockDiagonal(int(input_size * proj_factor), self.hidden_size, num_heads)\n","        self.Wk = BlockDiagonal(int(input_size * proj_factor), self.hidden_size, num_heads)\n","        self.Wv = BlockDiagonal(int(input_size * proj_factor), self.hidden_size, num_heads)\n","        self.Wi = nn.Linear(int(input_size * proj_factor), self.hidden_size)\n","        self.Wf = nn.Linear(int(input_size * proj_factor), self.hidden_size)\n","        self.Wo = nn.Linear(int(input_size * proj_factor), self.hidden_size)\n","\n","        self.group_norm = nn.GroupNorm(num_heads, self.hidden_size)\n","\n","    def forward(self, x, prev_state):\n","        h_prev, c_prev, n_prev, m_prev = prev_state\n","\n","        h_prev = h_prev.to(x.device)\n","        c_prev = c_prev.to(x.device)\n","        n_prev = n_prev.to(x.device)\n","        m_prev = m_prev.to(x.device)\n","\n","        assert x.size(-1) == self.input_size\n","        x_norm = self.layer_norm(x)\n","        x_up_left = self.up_proj_left(x_norm)\n","        x_up_right = self.up_proj_right(x_norm)\n","\n","        x_conv = F.silu(self.causal_conv(x_up_left.unsqueeze(1)).squeeze(1))\n","        x_skip = self.skip_connection(x_conv)\n","\n","        q = self.Wq(x_conv)\n","        k = self.Wk(x_conv) / (self.head_size ** 0.5)\n","        v = self.Wv(x_up_left)\n","\n","        i_tilde = self.Wi(x_conv)\n","        f_tilde = self.Wf(x_conv)\n","        o = torch.sigmoid(self.Wo(x_up_left))\n","\n","        m_t = torch.max(f_tilde + m_prev, i_tilde)\n","        i = torch.exp(i_tilde - m_t)\n","        f = torch.exp(f_tilde + m_prev - m_t)\n","\n","        c_t = f * c_prev + i * (v * k) # v @ k.T\n","        n_t = f * n_prev + i * k\n","        h_t = o * (c_t * q) / torch.max(torch.abs(n_t.T @ q), 1)[0] # o * (c @ q) / max{|n.T @ q|, 1}\n","\n","        output = h_t\n","        output_norm = self.group_norm(output)\n","        output = output_norm + x_skip\n","        output = output * F.silu(x_up_right)\n","        output = self.down_proj(output)\n","        final_output = output + x\n","\n","        return final_output, (h_t, c_t, n_t, m_t)\n","\n","class mLSTM(nn.Module):\n","    # Add bias, dropout, bidirectional\n","    def __init__(self, input_size, head_size, num_heads, num_layers=1, batch_first=False, proj_factor=2):\n","        super(mLSTM, self).__init__()\n","        self.input_size = input_size\n","        self.head_size = head_size\n","        self.hidden_size = head_size * num_heads\n","        self.num_heads = num_heads\n","        self.num_layers = num_layers\n","        self.batch_first = batch_first\n","        self.proj_factor_slstm = proj_factor\n","\n","        self.layers = nn.ModuleList([mLSTMBlock(input_size, head_size, num_heads, proj_factor) for _ in range(num_layers)])\n","\n","    def forward(self, x, state=None):\n","        assert x.ndim == 3\n","        if self.batch_first: x = x.transpose(0, 1)\n","        seq_len, batch_size, _ = x.size()\n","\n","        if state is not None:\n","            state = torch.stack(list(state)).to(x.device)\n","            assert state.ndim == 4\n","            num_hidden, state_num_layers, state_batch_size, state_input_size = state.size()\n","            assert num_hidden == 4\n","            assert state_num_layers == self.num_layers\n","            assert state_batch_size == batch_size\n","            assert state_input_size == self.input_size\n","            state = state.transpose(0, 1)\n","        else:\n","            state = torch.zeros(self.num_layers, 4, batch_size, self.hidden_size, device=x.device)\n","\n","        output = []\n","        for t in range(seq_len):\n","            x_t = x[t]\n","            for layer in range(self.num_layers):\n","                x_t, state_tuple = self.layers[layer](x_t, tuple(state[layer].clone()))\n","                state[layer] = torch.stack(list(state_tuple))\n","            output.append(x_t)\n","\n","        output = torch.stack(output)\n","        if self.batch_first:\n","            output = output.transpose(0, 1)\n","        state = tuple(state.transpose(0, 1))\n","        return output, state\n","\n","class xLSTM(nn.Module):\n","    # Add bias, dropout, bidirectional\n","    def __init__(self, input_size, head_size, num_heads, layers, batch_first=False, proj_factor_slstm=4/3, proj_factor_mlstm=2):\n","        super(xLSTM, self).__init__()\n","        self.input_size = input_size\n","        self.head_size = head_size\n","        self.hidden_size = head_size * num_heads\n","        self.num_heads = num_heads\n","        self.layers = layers\n","        self.num_layers = len(layers)\n","        self.batch_first = batch_first\n","        self.proj_factor_slstm = proj_factor_slstm\n","        self.proj_factor_mlstm = proj_factor_mlstm\n","\n","        self.layers = nn.ModuleList()\n","        for layer_type in layers:\n","            if layer_type == 's':\n","                layer = sLSTMBlock(input_size, head_size, num_heads, proj_factor_slstm)\n","            elif layer_type == 'm':\n","                layer = mLSTMBlock(input_size, head_size, num_heads, proj_factor_mlstm)\n","            else:\n","                raise ValueError(f\"Invalid layer type\")\n","            self.layers.append(layer)\n","\n","    def forward(self, x, state=None):\n","        assert x.ndim == 3\n","        if self.batch_first: x = x.transpose(0, 1)\n","        seq_len, batch_size, _ = x.size()\n","\n","        if state is not None:\n","            state = torch.stack(list(state)).to(x.device)\n","            assert state.ndim == 4\n","            num_hidden, state_num_layers, state_batch_size, state_input_size = state.size()\n","            assert num_hidden == 4\n","            assert state_num_layers == self.num_layers\n","            assert state_batch_size == batch_size\n","            assert state_input_size == self.input_size\n","            state = state.transpose(0, 1)\n","        else:\n","            state = torch.zeros(self.num_layers, 4, batch_size, self.hidden_size, device=x.device)\n","\n","        output = []\n","        for t in range(seq_len):\n","            x_t = x[t]\n","            for layer in range(self.num_layers):\n","                x_t, state_tuple = self.layers[layer](x_t, tuple(state[layer].clone()))\n","                state[layer] = torch.stack(list(state_tuple))\n","            output.append(x_t)\n","\n","        output = torch.stack(output)\n","        if self.batch_first:\n","            output = output.transpose(0, 1)\n","        state = tuple(state.transpose(0, 1))\n","        return output, state"]},{"cell_type":"code","execution_count":null,"id":"9a0959bc-ef24-41a3-a35b-efa336655f62","metadata":{"id":"9a0959bc-ef24-41a3-a35b-efa336655f62"},"outputs":[],"source":["# Инициализация модели\n","model = xLSTM(input_size=512, head_size=512, num_heads=2, layers=\"msm\")\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Генерация случайных входных данных\n","input_tensor = torch.randn(32, 128, 512)\n","target_tensor = torch.randn(32, 128, 512)\n","\n","# Обучение модели\n","num_epochs = 20\n","for epoch in range(num_epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    output = model(input_tensor)\n","    if isinstance(output, tuple):\n","        output = output[0]\n","    loss = criterion(output, target_tensor)\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.6f}\")\n","\n","# Проверка модели\n","with torch.no_grad():\n","    output = model(input_tensor)\n","    if isinstance(output, tuple):\n","        output = output[0]\n","\n","    print(\"Output shape after training:\", output.shape)"]},{"cell_type":"markdown","id":"343040ed-d7c0-44dc-b864-979db12bc7fc","metadata":{"id":"343040ed-d7c0-44dc-b864-979db12bc7fc"},"source":["## Домашнее задание: Обучение и визуализация xLSTM\n","\n","### Цель задания\n","\n","1. Обучить простую модель семейства xLSTM для обработки данных\n","2. Визуализировать веса внимания и интерпретировать их\n","3. Сделать выводы о том, как модель воспринимает данные и принимает решения?"]},{"cell_type":"code","execution_count":null,"id":"fcfdb2e7-ba8f-411f-92c2-a6977e4b3456","metadata":{"id":"fcfdb2e7-ba8f-411f-92c2-a6977e4b3456"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}